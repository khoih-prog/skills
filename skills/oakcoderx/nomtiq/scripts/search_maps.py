#!/usr/bin/env python3
"""
Nomtiq å°é¥­ç¥¨ - åœ°å›¾ API æœç´¢
ä¸­å›½ï¼šé«˜å¾·åœ°å›¾ POI æœç´¢ï¼ˆç»“æ„åŒ–æ•°æ®ï¼Œå…è´¹ 5000æ¬¡/å¤©ï¼‰
æµ·å¤–ï¼šSerper Google Mapsï¼ˆç»“æ„åŒ–æ•°æ®ï¼Œæœ‰è¯„åˆ†/è¥ä¸šæ—¶é—´/ä»·æ ¼æ¡£æ¬¡ï¼‰

ç”¨æ³•:
  python3 search_maps.py "çº¦ä¼šé¤å… ç¯å¢ƒå¥½" --city é•¿æ²™ --district å²³éº“åŒº
  python3 search_maps.py "romantic restaurant" --city "Changsha" --mode overseas
  python3 search_maps.py "dim sum" --city "New York" --mode overseas
"""

import sys, json, argparse, os, re, subprocess
from pathlib import Path
from urllib.request import urlopen, Request
from urllib.parse import urlencode
from urllib.error import URLError
from datetime import date

WORKSPACE = Path(__file__).parent.parent.parent.parent
DATA_DIR = Path(__file__).parent.parent / 'data'
RATE_LIMIT_FILE = DATA_DIR / 'rate_limit.json'
DAILY_LIMIT = 10

# â”€â”€ å†…ç½®é«˜å¾· Keyï¼ˆæ–°ç”¨æˆ·é›¶é…ç½®ï¼Œæ¯å¤©é™ 10 æ¬¡ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç”¨æˆ·å¯ç”³è¯·è‡ªå·±çš„ key é…ç½®åˆ°ç¯å¢ƒå˜é‡ AMAP_KEYï¼Œæ— é™ä½¿ç”¨
BUILTIN_AMAP_KEY = '15446a418e6fedc8b4e5e0de4942598e'
AMAP_KEY = os.environ.get('AMAP_KEY', BUILTIN_AMAP_KEY)

def check_rate_limit() -> bool:
    """æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ¯æ—¥é™æµ"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    today = str(date.today())
    if RATE_LIMIT_FILE.exists():
        data = json.loads(RATE_LIMIT_FILE.read_text())
        if data.get('date') == today:
            return data.get('count', 0) < DAILY_LIMIT
    return True

def increment_rate_limit():
    today = str(date.today())
    data = {'date': today, 'count': 1}
    if RATE_LIMIT_FILE.exists():
        old = json.loads(RATE_LIMIT_FILE.read_text())
        if old.get('date') == today:
            data['count'] = old.get('count', 0) + 1
    RATE_LIMIT_FILE.write_text(json.dumps(data))


# â”€â”€ é«˜å¾·åœ°å›¾ POI æœç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AMAP_KEY = os.environ.get('AMAP_KEY', '')

# é«˜å¾·åŸå¸‚ä»£ç æ˜ å°„ï¼ˆå¸¸ç”¨åŸå¸‚ï¼‰
AMAP_CITY_CODES = {
    'åŒ—äº¬': '010', 'ä¸Šæµ·': '021', 'å¹¿å·': '020', 'æ·±åœ³': '0755',
    'æˆéƒ½': '028', 'æ­å·': '0571', 'æ­¦æ±‰': '027', 'è¥¿å®‰': '029',
    'å—äº¬': '025', 'é‡åº†': '023', 'é•¿æ²™': '0731', 'å¤©æ´¥': '022',
    'è‹å·': '0512', 'éƒ‘å·': '0371', 'é’å²›': '0532', 'å¦é—¨': '0592',
}

# é«˜å¾·é¤é¥® POI ç±»å‹ä»£ç 
AMAP_FOOD_TYPES = '050000'  # é¤é¥®æœåŠ¡å¤§ç±»


def search_amap(query: str, city: str = '', district: str = '',
                max_results: int = 20) -> list:
    """é«˜å¾·åœ°å›¾ POI æœç´¢"""
    # é™æµæ£€æŸ¥ï¼ˆä»…å½“ä½¿ç”¨å†…ç½® key æ—¶ï¼‰
    if AMAP_KEY == BUILTIN_AMAP_KEY:
        if not check_rate_limit():
            print("âŒ ä»Šå¤©çš„å…è´¹æ¬¡æ•°ç”¨å®Œäº†ï¼ˆ10æ¬¡/å¤©ï¼‰", file=sys.stderr)
            print("ç”³è¯·è‡ªå·±çš„é«˜å¾· key å¯ä»¥æ— é™ç”¨ï¼š", file=sys.stderr)
            print("  https://lbs.amap.com â†’ åˆ›å»ºåº”ç”¨ â†’ WebæœåŠ¡", file=sys.stderr)
            return []
        increment_rate_limit()

    if not AMAP_KEY:
        print("âš ï¸  æœªé…ç½® AMAP_KEYï¼Œè·³è¿‡é«˜å¾·æœç´¢", file=sys.stderr)
        return []

    city_code = AMAP_CITY_CODES.get(city, city)
    keywords = f"{district} {query}".strip() if district else query

    params = {
        'key': AMAP_KEY,
        'keywords': keywords,
        'types': AMAP_FOOD_TYPES,
        'city': city_code,
        'citylimit': 'true',
        'offset': min(max_results, 25),
        'page': 1,
        'extensions': 'all',  # è¿”å›è¯¦ç»†ä¿¡æ¯ï¼ˆè¯„åˆ†ã€è¥ä¸šæ—¶é—´ç­‰ï¼‰
        'output': 'json',
    }

    url = f"https://restapi.amap.com/v3/place/text?{urlencode(params)}"
    try:
        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read())

        if data.get('status') != '1':
            print(f"é«˜å¾· API é”™è¯¯: {data.get('info')}", file=sys.stderr)
            return []

        pois = data.get('pois', [])
        results = []
        for poi in pois:
            r = _parse_amap_poi(poi)
            if r:
                results.append(r)
        return results

    except Exception as e:
        print(f"é«˜å¾·æœç´¢å‡ºé”™: {e}", file=sys.stderr)
        return []


def _parse_amap_poi(poi: dict) -> dict | None:
    """è§£æé«˜å¾· POI æ•°æ®"""
    name = poi.get('name', '')
    if not name:
        return None

    # è¿‡æ»¤éé¤å…ï¼ˆå¿«é¤/ä¾¿åˆ©åº—/è¶…å¸‚ï¼‰
    type_name = poi.get('type', '')
    # é«˜å¾· type æ ¼å¼ï¼š"é¤é¥®æœåŠ¡;ä¸­é¤å…;æ¹˜èœ" â†’ å–æœ€åä¸€æ®µ
    type_display = type_name.split(';')[-1] if ';' in type_name else type_name
    poi['_type_display'] = type_display
    skip_types = ['å¿«é¤', 'ä¾¿åˆ©åº—', 'è¶…å¸‚', 'é£Ÿå ‚', 'å°åƒ', 'æ—©é¤']
    if any(t in type_name for t in skip_types):
        return None

    # è¯„åˆ†ï¼ˆé«˜å¾·è¯„åˆ† 1-5 æ˜Ÿï¼Œbiz_ext é‡Œï¼‰
    biz_ext = poi.get('biz_ext', {})
    rating = None
    rating_raw = biz_ext.get('rating', '')
    if rating_raw and rating_raw != 'none':
        try:
            rating = float(rating_raw)
        except:
            pass

    # äººå‡æ¶ˆè´¹
    avg_cost = None
    cost_raw = biz_ext.get('cost', '')
    if cost_raw and cost_raw != 'none':
        try:
            avg_cost = int(float(cost_raw))
        except:
            pass

    # è¥ä¸šæ—¶é—´
    open_time = biz_ext.get('open_time', '') or poi.get('business_area', '')

    # åœ°å€
    address = poi.get('address', '')
    if isinstance(address, list):
        address = ''.join(address)

    # åŒºåŸŸ
    pname = poi.get('pname', '')   # çœ
    cityname = poi.get('cityname', '')  # å¸‚
    adname = poi.get('adname', '')  # åŒº

    # èœç³»ï¼ˆä» type å­—æ®µæå–ï¼‰
    cuisines = _extract_cuisines_cn(type_name + ' ' + name)
    type_display = poi.get('_type_display', type_name)

    return {
        'name': name,
        'score': rating,
        'avg_price': avg_cost,
        'cuisines': cuisines,
        'type': type_display,
        'address': address,
        'district': adname,
        'city': cityname,
        'open_time': open_time,
        'tel': poi.get('tel', ''),
        'source': 'amap',
        'sources': ['amap'],
        'cross_verified': False,
        'amap_id': poi.get('id', ''),
        'location': poi.get('location', ''),  # ç»çº¬åº¦
    }


# â”€â”€ ç™¾åº¦åœ°å›¾ POI æœç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BMAP_KEY = os.environ.get('BMAP_KEY', '')


def search_bmap(query: str, city: str = '', district: str = '',
                max_results: int = 20) -> list:
    """ç™¾åº¦åœ°å›¾åœ°ç‚¹æ£€ç´¢"""
    if not BMAP_KEY:
        print("âš ï¸  æœªé…ç½® BMAP_KEYï¼Œè·³è¿‡ç™¾åº¦æœç´¢", file=sys.stderr)
        return []

    region = district or city

    params = {
        'ak': BMAP_KEY,
        'query': query,
        'region': region,
        'output': 'json',
        'page_size': min(max_results, 20),
        'page_num': 0,
        'scope': 2,  # è¿”å›è¯¦ç»†ä¿¡æ¯
        'filter': 'industry_type:cater',  # åªè¿”å›é¤é¥®
    }

    url = f"https://api.map.baidu.com/place/v2/search?{urlencode(params)}"
    try:
        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read())

        if data.get('status') != 0:
            print(f"ç™¾åº¦åœ°å›¾ API é”™è¯¯: {data.get('message')}", file=sys.stderr)
            return []

        results = []
        for place in data.get('results', []):
            r = _parse_bmap_place(place)
            if r:
                results.append(r)
        return results

    except Exception as e:
        print(f"ç™¾åº¦æœç´¢å‡ºé”™: {e}", file=sys.stderr)
        return []


def _parse_bmap_place(place: dict) -> dict | None:
    name = place.get('name', '')
    if not name:
        return None

    detail = place.get('detail_info', {})

    rating = None
    rating_raw = detail.get('overall_rating', '')
    if rating_raw:
        try:
            rating = float(rating_raw)
        except:
            pass

    avg_cost = None
    cost_raw = detail.get('price', '')
    if cost_raw:
        try:
            avg_cost = int(float(cost_raw))
        except:
            pass

    tag = detail.get('tag', '')
    cuisines = _extract_cuisines_cn(tag + ' ' + name)

    return {
        'name': name,
        'score': rating,
        'avg_price': avg_cost,
        'cuisines': cuisines,
        'type': tag,
        'address': place.get('address', ''),
        'district': place.get('area', ''),
        'city': '',
        'tel': place.get('telephone', ''),
        'source': 'bmap',
        'sources': ['bmap'],
        'cross_verified': False,
        'bmap_uid': place.get('uid', ''),
    }


# â”€â”€ Serper Google Mapsï¼ˆæµ·å¤–ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def search_serper_maps(query: str, city: str = '', max_results: int = 20) -> list:
    """Serper Google Maps æœç´¢ï¼ˆæµ·å¤–åœºæ™¯ï¼‰"""
    full_query = f"{query} {city}".strip()

    try:
        result = subprocess.run(
            ['serperV', 'search', '-q', full_query, '-t', 'maps',
             '-l', str(max_results)],
            capture_output=True, text=True, timeout=20
        )
        if result.returncode != 0:
            print(f"Serper maps å‡ºé”™: {result.stderr[:100]}", file=sys.stderr)
            return []

        data = json.loads(result.stdout)
        places = data.get('places', [])

        results = []
        for p in places:
            r = _parse_serper_place(p)
            if r:
                results.append(r)
        return results

    except Exception as e:
        print(f"Serper maps å‡ºé”™: {e}", file=sys.stderr)
        return []


def _parse_serper_place(p: dict) -> dict | None:
    name = p.get('title', '')
    if not name:
        return None

    # è¿‡æ»¤è¯„åˆ†å¤ªä½æˆ–è¯„è®ºå¤ªå°‘çš„
    rating = p.get('rating')
    rating_count = p.get('ratingCount', 0) or 0

    # ä»·æ ¼æ¡£æ¬¡ $ $$ $$$ $$$$
    price_level = len(p.get('priceLevel', '')) if p.get('priceLevel') else None

    type_str = p.get('type', '') or ''
    if isinstance(p.get('types'), list):
        type_str = ', '.join(p['types'])

    cuisines = _extract_cuisines_en(type_str + ' ' + name)

    return {
        'name': name,
        'score': rating,
        'rating_count': rating_count,
        'price_level': price_level,
        'cuisines': cuisines,
        'type': type_str,
        'address': p.get('address', ''),
        'open_now': p.get('openingHours') is not None,
        'opening_hours': p.get('openingHours', {}),
        'thumbnail': p.get('thumbnailUrl', ''),
        'source': 'google_maps',
        'sources': ['google_maps'],
        'cross_verified': False,
        'place_id': p.get('placeId', ''),
        'cid': p.get('cid', ''),
    }


# â”€â”€ åŒæºåˆå¹¶ï¼ˆé«˜å¾· + ç™¾åº¦ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def merge_cn_sources(amap_results: list, bmap_results: list) -> list:
    """åˆå¹¶é«˜å¾·å’Œç™¾åº¦ç»“æœï¼ŒåŒååº—é“ºäº¤å‰éªŒè¯"""
    merged = {}

    for r in amap_results:
        key = _normalize_name(r['name'])
        merged[key] = r

    for r in bmap_results:
        key = _normalize_name(r['name'])
        if key in merged:
            merged[key]['cross_verified'] = True
            merged[key]['sources'].append('bmap')
            # è¡¥å……ç¼ºå¤±çš„è¯„åˆ†
            if not merged[key].get('score') and r.get('score'):
                merged[key]['score'] = r['score']
            if not merged[key].get('avg_price') and r.get('avg_price'):
                merged[key]['avg_price'] = r['avg_price']
        else:
            merged[key] = r

    return list(merged.values())


def _normalize_name(name: str) -> str:
    """æ ‡å‡†åŒ–åº—åç”¨äºåŒ¹é…ï¼ˆå»æ‰æ‹¬å·å†…å®¹ã€ç©ºæ ¼ï¼‰"""
    name = re.sub(r'[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]', '', name)
    name = re.sub(r'\s+', '', name)
    return name.lower()


# â”€â”€ èœç³»æå– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _extract_cuisines_cn(text: str) -> list:
    mapping = {
        'æ¹˜èœ': ['æ¹˜èœ', 'æ¹–å—èœ', 'æ¹˜å¼'],
        'ç²¤èœ': ['ç²¤èœ', 'å¹¿ä¸œèœ', 'é¡ºå¾·', 'æ¸¯å¼', 'çƒ§å‘³', 'æ—©èŒ¶'],
        'å·èœ': ['å·èœ', 'å››å·', 'éº»è¾£', 'ç«é”…'],
        'æ—¥æ–™': ['æ—¥æ–™', 'æ—¥æœ¬æ–™ç†', 'å¯¿å¸', 'åˆºèº«', 'å±…é…’å±‹', 'omakase'],
        'è¥¿é¤': ['è¥¿é¤', 'æ³•é¤', 'æ„å¤§åˆ©', 'ç‰›æ’', 'bistro', 'Bistro'],
        'äº‘å—èœ': ['äº‘å—', 'æ»‡èœ', 'ç±³çº¿'],
        'æµ·é²œ': ['æµ·é²œ', 'æ°´äº§', 'é±¼'],
        'çƒ§çƒ¤': ['çƒ§çƒ¤', 'çƒ¤è‚‰', 'ä¸²ä¸²'],
        'ç§æˆ¿èœ': ['ç§æˆ¿', 'ç§å¨', 'å®¶å®´'],
    }
    result = []
    for cuisine, kws in mapping.items():
        if any(kw in text for kw in kws):
            result.append(cuisine)
    return result


def _extract_cuisines_en(text: str) -> list:
    mapping = {
        'Chinese': ['chinese', 'hunan', 'cantonese', 'sichuan', 'dim sum'],
        'Japanese': ['japanese', 'sushi', 'ramen', 'izakaya'],
        'Korean': ['korean', 'bbq'],
        'Italian': ['italian', 'pizza', 'pasta'],
        'French': ['french', 'bistro', 'brasserie'],
        'Thai': ['thai'],
        'Indian': ['indian', 'curry'],
        'American': ['american', 'burger', 'steakhouse'],
        'Western': ['western', 'european'],
    }
    text_lower = text.lower()
    result = []
    for cuisine, kws in mapping.items():
        if any(kw in text_lower for kw in kws):
            result.append(cuisine)
    return result


# â”€â”€ ä¸»å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def search_maps(query: str, city: str = '', district: str = '',
                mode: str = 'china', max_results: int = 20) -> list:
    """ç»Ÿä¸€åœ°å›¾æœç´¢å…¥å£"""
    if mode == 'china':
        print(f"ğŸ—ºï¸  é«˜å¾·åœ°å›¾æœç´¢...", file=sys.stderr)
        amap = search_amap(query, city, district, max_results)
        print(f"   é«˜å¾·: {len(amap)} å®¶", file=sys.stderr)

        print(f"ğŸ—ºï¸  ç™¾åº¦åœ°å›¾æœç´¢...", file=sys.stderr)
        bmap = search_bmap(query, city or district, district, max_results)
        print(f"   ç™¾åº¦: {len(bmap)} å®¶", file=sys.stderr)

        merged = merge_cn_sources(amap, bmap)
        print(f"   åˆå¹¶å: {len(merged)} å®¶ï¼ˆ{sum(1 for r in merged if r.get('cross_verified'))} å®¶åŒæºéªŒè¯ï¼‰", file=sys.stderr)
        return merged

    else:  # overseas
        print(f"ğŸ—ºï¸  Google Maps æœç´¢...", file=sys.stderr)
        results = search_serper_maps(f"{query} restaurant", city, max_results)
        print(f"   æ‰¾åˆ°: {len(results)} å®¶", file=sys.stderr)
        return results


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Nomtiq - åœ°å›¾ API æœç´¢')
    parser.add_argument('query', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--city', default='', help='åŸå¸‚')
    parser.add_argument('--district', default='', help='åŒºåŸŸï¼ˆå¦‚ï¼šå²³éº“åŒºï¼‰')
    parser.add_argument('--mode', choices=['china', 'overseas'], default='china')
    parser.add_argument('--max', type=int, default=20)
    parser.add_argument('--json', action='store_true')
    args = parser.parse_args()

    results = search_maps(args.query, args.city, args.district, args.mode, args.max)

    if args.json:
        print(json.dumps(results, ensure_ascii=False, indent=2))
        sys.exit(0)

    print(f"\næ‰¾åˆ° {len(results)} å®¶é¤å…\n")
    for i, r in enumerate(results[:15], 1):
        score = f"â­{r['score']}" if r.get('score') else ''
        price_cn = f"Â¥{r['avg_price']}" if r.get('avg_price') else ''
        price_en = '$' * r['price_level'] if r.get('price_level') else ''
        price = price_cn or price_en
        cuisines = '/'.join(r.get('cuisines', [])[:2])
        verified = 'âœ…åŒæº' if r.get('cross_verified') else ''
        district = r.get('district', '')
        info = ' | '.join(p for p in [price, score, cuisines, district, verified] if p)
        print(f"{i}. {r['name']}")
        if info:
            print(f"   {info}")
        if r.get('address'):
            print(f"   ğŸ“ {r['address'][:60]}")
        print()


# â”€â”€ ç¤¾äº¤åª’ä½“äº¤å‰éªŒè¯ï¼ˆå…¬å¼€æ•°æ®ï¼Œæ— éœ€ç™»å½•ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

WORKSPACE = Path(__file__).parent.parent.parent.parent
HUB_SCRIPT = str(WORKSPACE / 'skills/search-hub/scripts/hub.py')


def cross_verify_social(restaurants: list, max_verify: int = 5) -> list:
    """
    ç”¨ Serper æœç´¢å…¬å¼€ç¤¾äº¤åª’ä½“æ•°æ®åšäº¤å‰éªŒè¯ã€‚
    ä¸éœ€è¦ç”¨æˆ·ç™»å½•ä»»ä½•è´¦å·ã€‚
    å›½å†…ï¼šæœå°çº¢ä¹¦ + å¤§ä¼—ç‚¹è¯„å…¬å¼€é¡µé¢
    """
    import subprocess

    print(f"ğŸ” ç¤¾äº¤åª’ä½“äº¤å‰éªŒè¯ï¼ˆtop {max_verify}ï¼‰...", file=sys.stderr)

    for r in restaurants[:max_verify]:
        name = r['name']
        clean_name = re.sub(r'[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]', '', name).strip()

        # æœå°çº¢ä¹¦å…¬å¼€é¡µé¢
        xhs_query = f'site:xiaohongshu.com {clean_name} æ¢åº—'
        try:
            result = subprocess.run(
                ['python3.13', HUB_SCRIPT, 'search', xhs_query, '-t', 'text', '-l', '3'],
                capture_output=True, text=True, timeout=20
            )
            if result.returncode == 0:
                data = json.loads(result.stdout)
                hits = data.get('results', [])
                if hits:
                    r['xhs_verified'] = True
                    snippets = ' '.join(
                        h.get('snippet', '') + h.get('title', '') for h in hits
                    )
                    neg_words = ['éš¾åƒ', 'è¸©é›·', 'å¤±æœ›', 'ä¸æ¨è', 'å·®è¯„', 'å‘', 'åæ‚”', 'ä¸€èˆ¬']
                    pos_words = ['å¥½åƒ', 'æ¨è', 'å¿…å»', 'è¶…æ£’', 'å–œæ¬¢', 'å€¼å¾—', 'å®è—', 'æƒŠå–œ']
                    neg = sum(1 for w in neg_words if w in snippets)
                    pos = sum(1 for w in pos_words if w in snippets)
                    r['xhs_sentiment'] = 'negative' if neg > pos else ('positive' if pos > 0 else 'neutral')
                    print(f"   âœ… {clean_name}: å°çº¢ä¹¦ {len(hits)} æ¡ï¼Œæƒ…æ„Ÿ={r['xhs_sentiment']}", file=sys.stderr)
                else:
                    print(f"   â€” {clean_name}: å°çº¢ä¹¦æ— è®°å½•", file=sys.stderr)
        except Exception as e:
            print(f"   âš ï¸  {clean_name} éªŒè¯å‡ºé”™: {e}", file=sys.stderr)

    return restaurants




# è¿é”å“ç‰Œé»‘åå•ï¼ˆé¥­å¡æ¨¡å¼è¿‡æ»¤ï¼‰
CHAIN_BRANDS = [
    'éº¦å½“åŠ³', 'è‚¯å¾·åŸº', 'å¿…èƒœå®¢', 'æ˜Ÿå·´å…‹', 'æµ·åº•æ', 'è¥¿è´', 'å¤–å©†å®¶',
    'ç»¿èŒ¶', 'å¤ªäºŒ', 'ä¹æ¯›ä¹', 'å‘·å“º', 'å°é¾™å', 'å¤§é¾™ç‡š', 'å·´å¥´',
    'çœ‰å·ä¸œå¡', 'å…¨èšå¾·', 'ä¾¿å®œåŠ', 'ä¸œæ¥é¡º', 'æ—ºé¡ºé˜',
    'è¨è‰äºš', 'å¿…èƒœå®¢', 'æ£’çº¦ç¿°', 'æ±‰å ¡ç‹', 'å¾·å…‹å£«',
]

def fancard_filter(results: list, budget_low: int = 60, budget_high: int = 250) -> list:
    """
    é¥­å¡æ¨¡å¼è¿‡æ»¤ï¼š
    - è¯„åˆ† >= 4.3ï¼ˆé™ˆæ™“å¿å®šå¾‹ï¼šè¡—è¾¹å°åº— 3.5-4 æ‰çœŸå®ï¼Œè¿™é‡Œç”¨é«˜å¾·è¯„åˆ†ä½“ç³»ï¼‰
    - äººå‡åœ¨é¢„ç®—èŒƒå›´å†…
    - éè¿é”å“ç‰Œ
    - éå¿«é¤/é£Ÿå ‚ç±»å‹
    """
    filtered = []
    for r in results:
        # è¯„åˆ†è¿‡æ»¤
        score = r.get('score')
        if score and score < 4.3:
            continue

        # äººå‡è¿‡æ»¤
        price = r.get('avg_price')
        if price:
            if price < budget_low or price > budget_high:
                continue

        # è¿é”è¿‡æ»¤
        name = r.get('name', '')
        if any(brand in name for brand in CHAIN_BRANDS):
            continue

        # ç±»å‹è¿‡æ»¤ï¼ˆå¿«é¤/é£Ÿå ‚ï¼‰
        type_name = r.get('type', '')
        skip = ['å¿«é¤', 'é£Ÿå ‚', 'ä¾¿åˆ©', 'è¶…å¸‚', 'æ—©é¤', 'é¢åŒ…', 'ç”œå“', 'å¥¶èŒ¶', 'å’–å•¡']
        if any(s in type_name for s in skip):
            continue

        filtered.append(r)

    # æŒ‰è¯„åˆ†æ’åº
    filtered.sort(key=lambda x: (x.get('score') or 0), reverse=True)
    return filtered


def generate_fancard_blurb(r: dict, is_explorer: bool = False) -> str:
    """é¥­å¡æ¨¡å¼æ¨èè¯­ï¼ˆè§„åˆ™ç‰ˆï¼Œä¾› OpenClaw æ¶¦è‰²ï¼‰"""
    address = r.get('address', '') or ''
    cuisines = r.get('cuisines', [])
    price = r.get('avg_price', 0) or 0
    name = r.get('name', '')
    score = r.get('score', 0) or 0

    parts = []

    # èœç³»ç¨€ç¼ºæ€§ä¼˜å…ˆï¼ˆæ¯”ä½ç½®æ›´æœ‰ä¸ªæ€§ï¼‰
    rare = {
        'æ±Ÿè‹èœ': 'è‹å¸®èœåœ¨åŒ—äº¬ä¸å¤šè§',
        'é—½èœ': 'é—½èœåœ¨åŒ—äº¬å°‘è§ï¼Œå€¼å¾—è¯•',
        'äº‘å—èœ': 'äº‘å—èœçš„é¦™æ–™ç”¨å¾—è®²ç©¶',
        'ç§æˆ¿èœ': 'ç§æˆ¿èœï¼Œä¸æ˜¯è¿é”ï¼Œæœ‰è‡ªå·±çš„é£æ ¼',
        'ç²¤èœ': 'ç²¤èœè®²ç©¶é£Ÿææœ¬å‘³ï¼Œä¸é è°ƒæ–™',
    }
    # cuisines å­—æ®µ + type å­—æ®µéƒ½æ£€æŸ¥
    cuisine_text = cuisines + [r.get('type', '')]
    for c in cuisine_text:
        if c in rare:
            parts.append(rare[c])
            break

    # ä½ç½®æ„ŸçŸ¥ï¼ˆèœç³»æ²¡å‘½ä¸­æ—¶ç”¨ï¼‰
    if not parts:
        loc_hints = {
            '798': '798 è‰ºæœ¯åŒºé‡Œï¼Œç¯å¢ƒæœ‰è°ƒæ€§ï¼Œä¸¤ä¸ªäººåä¸‹æ¥ä¸ä¼šè§‰å¾—åµ',
            'ä¸½éƒ½': 'è—åœ¨ä¸½éƒ½èŠ±å›­é‡Œï¼Œå®‰é™ï¼Œä¸æ˜¯éšä¾¿å°±èƒ½æ‰¾åˆ°çš„åœ°æ–¹',
            'å°†å°': 'å°†å°çš„è€è¡—åŒºï¼Œä¸é æµé‡ï¼Œé å£ç¢‘',
            'è“è‰²æ¸¯æ¹¾': 'è“è‰²æ¸¯æ¹¾é‡Œï¼Œç¯å¢ƒå¥½ï¼Œé€‚åˆæ…¢æ…¢èŠ',
            'é…’ä»™æ¡¥': 'é…’ä»™æ¡¥çš„æœ¬åœ°é¦†å­ï¼Œå¼€äº†å¥½å‡ å¹´äº†',
            'ä¸‰é‡Œå±¯': 'ä¸‰é‡Œå±¯é‡Œçš„é¦†å­ï¼Œçƒ­é—¹ä½†ä¸å˜ˆæ‚',
            'æœ›äº¬': 'æœ›äº¬çš„è¡—è¾¹å°é¦†ï¼Œæœ¬åœ°äººå¸¸å»çš„é‚£ç§',
        }
        for loc, hint in loc_hints.items():
            if loc in address or loc in name:
                parts.append(hint)
                break

    # æ¢ç´¢æ¨èä¸“å±
    if is_explorer and not parts:
        parts.append('ä½ å¯èƒ½æ²¡å»è¿‡ï¼Œä½†å€¼å¾—è¯•ä¸€æ¬¡')

    # ç¤¾äº¤åª’ä½“å£ç¢‘ï¼ˆä¸æš´éœ²æŠ€æœ¯ç»†èŠ‚ï¼Œåªè¯´ç»“è®ºï¼‰
    if r.get('xhs_verified'):
        if r.get('xhs_sentiment') == 'negative':
            parts.append('ç•™ä¸ªå¿ƒçœ¼ï¼Œæœ‰å·®è¯„')
        else:
            # positive æˆ– neutral éƒ½ç®—æœ‰å£ç¢‘
            parts.append('å°çº¢ä¹¦æœ‰æ¢åº—ï¼Œæœ¬åœ°äººå»è¿‡')

    # ä»·æ ¼æ„ŸçŸ¥ï¼ˆä¸­å›½é€»è¾‘ï¼šä»·æ ¼è¶Šé«˜ï¼Œäººè¶Šå°‘ï¼Œè¶Šå®‰é™ï¼‰
    if price:
        if price >= 150 and not any('å®‰é™' in p for p in parts):
            parts.append('äººå‡è¿‡ç™¾äº”ï¼Œäººä¸å¤šï¼Œå®‰é™ï¼Œé€‚åˆèŠå¤©')
        elif price >= 80 and price < 150:
            parts.append('äººå‡ä¸€ç™¾å·¦å³ï¼Œä¸ä¼šæœ‰å‹åŠ›')
        elif price < 80:
            parts.append('ä»·æ ¼å®æƒ ï¼Œä½†å¯èƒ½çƒ­é—¹')

    if not parts and score >= 4.7:
        parts.append('4.7 åˆ†ï¼Œå£ç¢‘åœ¨é‚£é‡Œï¼Œä¸ç”¨å¤šè¯´')

    return 'ã€‚'.join(parts[:2]) + 'ã€‚' if parts else ''


def search_fancard(location: str, city: str = 'åŒ—äº¬',
                   budget_low: int = 80, budget_high: int = 300) -> list:
    """
    é¥­å¡æ¨¡å¼ä¸»å…¥å£ï¼šæ‰¾é€‚åˆä¸¤äººèŠå¤©çš„æœ¬åœ°å°é¦†
    location: åœ°ç‚¹ï¼ˆå¦‚"é…’ä»™æ¡¥"ï¼‰
    """
    # å¤šå…³é”®è¯æœç´¢ï¼Œè¦†ç›–ä¸åŒé£æ ¼
    queries = [
        f"ç‰¹è‰²é¤å…",
        f"å°é¦†",
        f"ç§æˆ¿èœ",
    ]

    all_results = {}
    for q in queries:
        results = search_amap(q, city, location, max_results=25)
        for r in results:
            key = _normalize_name(r['name'])
            if key not in all_results:
                all_results[key] = r

    merged = list(all_results.values())
    filtered = fancard_filter(merged, budget_low, budget_high)

    # ç¤¾äº¤åª’ä½“äº¤å‰éªŒè¯ï¼ˆtop 5ï¼Œå…¬å¼€æ•°æ®ï¼Œæ— éœ€ç™»å½•ï¼‰
    filtered = cross_verify_social(filtered, max_verify=5)

    # åŠ æ¨èè¯­
    for i, r in enumerate(filtered):
        r['blurb'] = generate_fancard_blurb(r, is_explorer=(i >= 2))

    return filtered
