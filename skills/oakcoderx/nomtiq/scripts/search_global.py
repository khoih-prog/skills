#!/usr/bin/env python3
"""
Nomtiq å°é¥­ç¥¨ - å…¨çƒæœç´¢ï¼ˆGoogle Places + Yelp + Redditï¼‰
ç”¨äºæµ·å¤–ç”¨æˆ·æˆ–æµ·å¤–åäººåœºæ™¯

ç”¨æ³•:
  python3 search_global.py "Italian restaurant Manhattan"
  python3 search_global.py "ramen Tokyo" --city Tokyo --source all
  python3 search_global.py "dim sum Flushing" --city "New York" --mode 2plus1
"""

import sys
import json
import argparse
import re
import os
import subprocess
from pathlib import Path

WORKSPACE = Path(__file__).parent.parent.parent.parent  # ~/.openclaw/workspace


def _hub_search(query: str, max_results: int = 20) -> list:
    """è°ƒç”¨ search-hub"""
    try:
        result = subprocess.run(
            ['python3.13', 'skills/search-hub/scripts/hub.py', 'search', query, '-t', 'text', '-l', str(max_results)],
            cwd=str(WORKSPACE),
            capture_output=True, text=True, timeout=30
        )
        if result.returncode != 0:
            return []
        data = json.loads(result.stdout)
        return data.get('results', [])
    except Exception as e:
        print(f"search-hub error: {e}", file=sys.stderr)
        return []


def search_google_places(query: str, city: str = '', max_results: int = 20) -> list:
    """æœç´¢ Google Placesï¼ˆé€šè¿‡ search-hub æœç´¢ Google Mapsï¼‰"""
    city_str = f' {city}' if city else ''
    search_query = f'site:maps.google.com OR site:google.com/maps {query}{city_str} restaurant'
    
    # åŒæ—¶æœ TripAdvisor ä½œä¸ºè¡¥å……
    tripadvisor_query = f'site:tripadvisor.com {query}{city_str} restaurant review'
    
    results = []
    for q in [search_query, tripadvisor_query]:
        raw = _hub_search(q, max_results // 2)
        for r in raw:
            parsed = _parse_global_result(r, source='google')
            if parsed:
                results.append(parsed)
    
    return results


def search_yelp(query: str, city: str = '', max_results: int = 20) -> list:
    """æœç´¢ Yelp"""
    city_str = f' {city}' if city else ''
    search_query = f'site:yelp.com {query}{city_str}'
    
    raw = _hub_search(search_query, max_results)
    results = []
    for r in raw:
        parsed = _parse_global_result(r, source='yelp')
        if parsed:
            results.append(parsed)
    return results


def search_reddit(query: str, city: str = '', max_results: int = 10) -> list:
    """æœç´¢ Reddit æœ¬åœ°æ¨èï¼ˆæœ€çœŸå®çš„æœ¬åœ°äººå£ç¢‘ï¼‰"""
    city_str = city or ''
    # æœç´¢åŸå¸‚ subreddit çš„é¤å…æ¨è
    search_query = f'site:reddit.com {query} {city_str} restaurant recommendation'
    
    raw = _hub_search(search_query, max_results)
    results = []
    for r in raw:
        parsed = _parse_reddit_result(r)
        if parsed:
            results.append(parsed)
    return results


def search_xiaohongshu_global(query: str, max_results: int = 10) -> list:
    """æœç´¢å°çº¢ä¹¦ï¼ˆæµ·å¤–åäººåœºæ™¯ï¼‰"""
    # å°çº¢ä¹¦ä¸Šæœ‰å¤§é‡æµ·å¤–åäººçš„æ¢åº—å†…å®¹
    search_query = f'site:xiaohongshu.com {query} æ¢åº— æµ·å¤–'
    
    raw = _hub_search(search_query, max_results)
    results = []
    for r in raw:
        results.append({
            'name': _extract_name_from_title(r.get('title', '')),
            'snippet': r.get('snippet', '')[:200],
            'url': r.get('url', ''),
            'source': 'xiaohongshu',
            'sentiment': _detect_sentiment(r.get('snippet', '')),
            'is_chinese_community': True,
        })
    return [r for r in results if r['name']]


def _parse_global_result(result: dict, source: str) -> dict | None:
    """è§£æå…¨çƒæœç´¢ç»“æœ"""
    title = result.get('title', '')
    snippet = result.get('snippet', '')
    url = result.get('url', '')
    combined = f"{title} {snippet}"

    # è¿‡æ»¤éé¤å…å†…å®¹
    food_keywords = ['restaurant', 'cafe', 'bistro', 'bar', 'kitchen', 'grill', 'eatery',
                     'dining', 'food', 'cuisine', 'menu', 'reservation', 'é¤å…', 'æ–™ç†']
    if not any(kw.lower() in combined.lower() for kw in food_keywords):
        return None

    # æå–è¯„åˆ†
    score = None
    # Yelp: "4.5 star rating" or "4.5/5"
    score_match = re.search(r'(\d\.?\d?)\s*(?:star|stars|/5|\s*out of 5)', combined, re.IGNORECASE)
    if score_match:
        score = float(score_match.group(1))
    # Google: "(4.5)" or "4.5 â˜…"
    if not score:
        score_match = re.search(r'[â˜…â­]\s*(\d\.?\d?)|(\d\.?\d?)\s*[â˜…â­]', combined)
        if score_match:
            score = float(score_match.group(1) or score_match.group(2))

    # æå–ä»·æ ¼æ¡£æ¬¡
    price_level = None
    if '$$$$' in combined: price_level = 4
    elif '$$$' in combined: price_level = 3
    elif '$$' in combined: price_level = 2
    elif '$' in combined: price_level = 1

    # æå–èœç³»
    cuisine_keywords = {
        'Chinese': ['chinese', 'dim sum', 'cantonese', 'sichuan', 'peking', 'dumpling', 'ä¸­é¤', 'ç²¤èœ'],
        'Japanese': ['japanese', 'sushi', 'ramen', 'izakaya', 'omakase', 'tempura'],
        'Italian': ['italian', 'pizza', 'pasta', 'trattoria', 'osteria'],
        'French': ['french', 'bistro', 'brasserie', 'croissant'],
        'Korean': ['korean', 'bbq', 'bibimbap', 'kimchi'],
        'Thai': ['thai', 'pad thai', 'curry'],
        'Mexican': ['mexican', 'taco', 'burrito', 'enchilada'],
        'Indian': ['indian', 'curry', 'tandoor', 'biryani'],
        'American': ['american', 'burger', 'bbq', 'steakhouse'],
    }
    cuisines = []
    for cuisine, kws in cuisine_keywords.items():
        if any(kw.lower() in combined.lower() for kw in kws):
            cuisines.append(cuisine)

    name = _extract_name_from_title(title)
    if not name:
        return None

    return {
        'name': name,
        'score': score,
        'price_level': price_level,
        'cuisines': cuisines,
        'snippet': snippet[:200],
        'url': url,
        'source': source,
        'sources': [source],
        'cross_verified': False,
        'reddit_mentioned': False,
    }


def _parse_reddit_result(result: dict) -> dict | None:
    """è§£æ Reddit ç»“æœ"""
    title = result.get('title', '')
    snippet = result.get('snippet', '')
    url = result.get('url', '')

    if 'reddit.com' not in url:
        return None

    # æå–æåˆ°çš„é¤å…åï¼ˆé€šå¸¸åœ¨å¼•å·æˆ–å¤§å†™ä¸­ï¼‰
    mentioned = []
    # å¼•å·å†…çš„åå­—
    quoted = re.findall(r'"([^"]{3,40})"', f"{title} {snippet}")
    mentioned.extend(quoted)

    return {
        'title': title[:100],
        'snippet': snippet[:300],
        'url': url,
        'source': 'reddit',
        'restaurants_mentioned': mentioned,
        'sentiment': _detect_sentiment(snippet),
        'is_local_recommendation': True,
    }


def _extract_name_from_title(title: str) -> str:
    """ä»æ ‡é¢˜æå–é¤å…å"""
    # "Restaurant Name - Yelp" â†’ "Restaurant Name"
    # "Restaurant Name | TripAdvisor" â†’ "Restaurant Name"
    # "Restaurant Name (City)" â†’ "Restaurant Name"
    name = re.split(r'\s*[-|Â·]\s*(?:Yelp|TripAdvisor|Google|Maps|Reviews?|Menu)', title)[0]
    name = re.sub(r'\s*\([^)]*\)\s*$', '', name)  # å»æ‰æ‹¬å·å†…å®¹
    name = name.strip()
    return name if len(name) > 2 else ''


def _detect_sentiment(text: str) -> str:
    """ç®€å•æƒ…æ„Ÿæ£€æµ‹"""
    positive = ['great', 'amazing', 'excellent', 'best', 'love', 'recommend', 'delicious',
                'å¥½åƒ', 'æ¨è', 'å¿…å»', 'è¶…æ£’', 'å–œæ¬¢', 'hidden gem', 'underrated']
    negative = ['bad', 'terrible', 'avoid', 'worst', 'disappointing', 'overrated',
                'éš¾åƒ', 'ä¸æ¨è', 'è¸©é›·', 'å¤±æœ›']
    text_lower = text.lower()
    pos = sum(1 for w in positive if w in text_lower)
    neg = sum(1 for w in negative if w in text_lower)
    if pos > neg: return 'positive'
    if neg > pos: return 'negative'
    return 'neutral'


def search_all_global(query: str, city: str = '', max_results: int = 20,
                      include_xhs: bool = False) -> list:
    """å…¨çƒä¸‰æºæœç´¢ï¼šGoogle Places + Yelp + Reddit äº¤å‰éªŒè¯"""
    print(f"ğŸŒ å…¨çƒæœç´¢ä¸­...", file=sys.stderr)

    # 1. Google Places / TripAdvisor
    print(f"  Google Places...", file=sys.stderr)
    google_results = search_google_places(query, city, max_results)
    print(f"  æ‰¾åˆ° {len(google_results)} å®¶", file=sys.stderr)

    # 2. Yelp
    print(f"  Yelp...", file=sys.stderr)
    yelp_results = search_yelp(query, city, max_results)
    print(f"  æ‰¾åˆ° {len(yelp_results)} å®¶", file=sys.stderr)

    # 3. Reddit æœ¬åœ°å£ç¢‘
    print(f"  Reddit...", file=sys.stderr)
    reddit_results = search_reddit(query, city, 10)
    print(f"  æ‰¾åˆ° {len(reddit_results)} æ¡è®¨è®º", file=sys.stderr)

    # 4. å°çº¢ä¹¦ï¼ˆæµ·å¤–åäººåœºæ™¯ï¼‰
    xhs_results = []
    if include_xhs:
        print(f"  å°çº¢ä¹¦ï¼ˆæµ·å¤–åäººï¼‰...", file=sys.stderr)
        xhs_results = search_xiaohongshu_global(query, 10)
        print(f"  æ‰¾åˆ° {len(xhs_results)} æ¡ç¬”è®°", file=sys.stderr)

    # åˆå¹¶ Google + Yelp
    merged = {}
    for r in google_results + yelp_results:
        name = r['name'].lower()
        if name in merged:
            merged[name]['sources'].append(r['source'])
            merged[name]['cross_verified'] = True
            # åˆå¹¶è¯„åˆ†ï¼ˆå–å¹³å‡ï¼‰
            if r.get('score') and merged[name].get('score'):
                merged[name]['score'] = (merged[name]['score'] + r['score']) / 2
            elif r.get('score'):
                merged[name]['score'] = r['score']
        else:
            merged[name] = r

    # Reddit äº¤å‰éªŒè¯
    for reddit_post in reddit_results:
        for mentioned in reddit_post.get('restaurants_mentioned', []):
            for key in merged:
                if mentioned.lower() in key or key in mentioned.lower():
                    merged[key]['reddit_mentioned'] = True
                    merged[key]['reddit_sentiment'] = reddit_post.get('sentiment')
                    break

    results = list(merged.values())

    # æµ·å¤–åäººï¼šå°çº¢ä¹¦åŠ åˆ†
    if xhs_results:
        for xhs in xhs_results:
            xhs_name = (xhs.get('name') or '').lower()
            for key in merged:
                if xhs_name and (xhs_name in key or key in xhs_name):
                    merged[key]['xhs_verified'] = True
                    merged[key]['sources'].append('xiaohongshu')

    return results


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Nomtiq - å…¨çƒé¤å…æœç´¢')
    parser.add_argument('query', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--city', default='', help='åŸå¸‚')
    parser.add_argument('--max', type=int, default=20, help='æœ€å¤§ç»“æœæ•°')
    parser.add_argument('--xhs', action='store_true', help='åŒ…å«å°çº¢ä¹¦ï¼ˆæµ·å¤–åäººæ¨¡å¼ï¼‰')
    parser.add_argument('--json', action='store_true', help='JSON è¾“å‡º')
    args = parser.parse_args()

    results = search_all_global(args.query, args.city, args.max, include_xhs=args.xhs)

    if args.json:
        print(json.dumps(results, ensure_ascii=False, indent=2))
    else:
        print(f"\nğŸŒ æ‰¾åˆ° {len(results)} å®¶é¤å…\n")
        for i, r in enumerate(results[:10], 1):
            score = f"â­{r['score']}" if r.get('score') else ''
            price = '$' * r['price_level'] if r.get('price_level') else ''
            cuisines = '/'.join(r.get('cuisines', [])[:2])
            verified = 'âœ…' if r.get('cross_verified') else ''
            reddit = 'ğŸ—£ï¸Reddit' if r.get('reddit_mentioned') else ''
            info = ' | '.join(p for p in [score, price, cuisines, verified, reddit] if p)
            print(f"{i}. {r['name']}")
            if info: print(f"   {info}")
            if r.get('snippet'): print(f"   {r['snippet'][:80]}")
            print()
