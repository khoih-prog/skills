#!/usr/bin/env python3
"""
å°é¥­å¡ - å¤§ä¼—ç‚¹è¯„æœç´¢ï¼ˆä½¿ç”¨ search-hubï¼‰
ç”¨æ³•:
  python3 search.py "ä¸‰é‡Œå±¯ åˆ›æ„èœ"
  python3 search.py "å›½è´¸ æ—¥æ–™ äººå‡ 500" --city åŒ—äº¬
  python3 search.py "æœé˜³ ç´ é£Ÿ" --max 50 --json
"""

import sys
import json
import argparse
import re
import os
import subprocess

def search_dianping(query: str, city: str = '', max_results: int = 50) -> list:
    """æœç´¢å¤§ä¼—ç‚¹è¯„ä¸Šçš„é¤å…ä¿¡æ¯ï¼ˆä½¿ç”¨ search-hubï¼‰
    
    Args:
        query: æœç´¢å…³é”®è¯
        city: åŸå¸‚å
        max_results: æœ€å¤§ç»“æœæ•°ï¼ˆé»˜è®¤ 50 å®¶ï¼‰
    """
    city_str = f' {city}' if city else ''
    search_query = f'site:dianping.com {query}{city_str} é¤å…'
    
    try:
        # è°ƒç”¨ search-hubï¼ˆç”¨ python3.13 é¿å…ç‰ˆæœ¬é—®é¢˜ï¼‰
        result = subprocess.run(
            ['python3.13', 'skills/search-hub/scripts/hub.py', 'search', search_query, '-t', 'text', '-l', str(max_results)],
            cwd='/Users/mac/.openclaw/workspace',
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode != 0:
            print(f"search-hub å‡ºé”™ï¼š{result.stderr}", file=sys.stderr)
            return []
        
        # è§£æ JSON è¾“å‡º
        data = json.loads(result.stdout)
        results = data.get('results', [])
        
        # è§£æé¤å…ä¿¡æ¯
        restaurants = []
        for r in results:
            parsed = parse_result(r)
            if parsed:
                restaurants.append(parsed)
        
        return restaurants
        
    except Exception as e:
        print(f"æœç´¢å‡ºé”™ï¼š{e}", file=sys.stderr)
        return []


def parse_result(result):
    """ä»æœç´¢ç»“æœä¸­è§£æé¤å…ä¿¡æ¯"""
    title = result.get('title', '')
    snippet = result.get('snippet', '')
    url = result.get('url', '') or result.get('link', '')
    combined = f"{title} {snippet}"

    # è¿‡æ»¤éå¤§ä¼—ç‚¹è¯„é“¾æ¥
    if 'dianping.com' not in url:
        return None

    # åˆ¤æ–­æ˜¯å¦é¤å…ç›¸å…³
    food_keywords = ['é¤å…', 'é¤é¦†', 'é¥­åº—', 'èœ', 'äººå‡', 'æ¨èèœ', 'å¥½åƒ', 'å‘³é“', 'æ–™ç†', 'ç«é”…', 'çƒ¤', 'åº—', 'é¦†']
    if not any(kw in combined for kw in food_keywords):
        return None

    # è¿‡æ»¤éé¤å…é¡µé¢
    skip_patterns = ['shopRank', 'pcChannelRanking', '/photos', '/album']
    if any(p in url for p in skip_patterns):
        return None

    non_food = ['æŒ‰æ‘©', 'è¶³æµ´', 'å…»ç”Ÿé¦†', 'ç¾å®¹', 'ç¾å‘', 'é…’åº—', 'KTV', 'å¥èº«']
    if any(nf in combined for nf in non_food) and not any(kw in combined for kw in food_keywords[:6]):
        return None

    # æå–äººå‡
    price_match = re.search(r'[äººå‡Â¥ï¿¥](\d+)', combined)
    avg_price = int(price_match.group(1)) if price_match else None

    # æå–åº—å
    shop_name = None
    name_match = re.search(r'ã€(.+?)ã€‘', title)
    if name_match:
        shop_name = name_match.group(1)
    elif '(' in title and 'å¤§ä¼—ç‚¹è¯„' not in title:
        shop_name = title.split(' - ')[0].split('|')[0].strip()

    # æå–è¯„åˆ†
    score_match = re.search(r'(\d\.\d)\s*åˆ†', combined)
    score = float(score_match.group(1)) if score_match else None

    # æå–èœç³»
    categories = []
    cat_keywords = {
        'ä¸­é¤': ['ä¸­é¤', 'äº¬èœ', 'é²èœ', 'å·èœ', 'ç²¤èœ', 'æ¹˜èœ', 'æµ™èœ', 'è‹èœ', 'å¾½èœ', 'é—½èœ'],
        'æ—¥æ–™': ['æ—¥æœ¬æ–™ç†', 'æ—¥æ–™', 'å¯¿å¸', 'åˆºèº«', 'å±…é…’å±‹', 'omakase'],
        'è¥¿é¤': ['è¥¿é¤', 'æ³•é¤', 'æ„å¤§åˆ©', 'è¥¿ç­ç‰™', 'ç‰›æ’'],
        'ç«é”…': ['ç«é”…', 'æ¶®è‚‰', 'æ¶®é”…'],
        'çƒ§çƒ¤': ['çƒ¤è‚‰', 'çƒ§çƒ¤', 'ç‚™å­'],
        'æ½®æ±•èœ': ['æ½®æ±•', 'ç ‚é”…ç²¥', 'ç‰›è‚‰ç«é”…'],
        'æµ·é²œ': ['æµ·é²œ', 'æ°´äº§', 'èŸ¹', 'è™¾', 'è´'],
    }
    for cat, kws in cat_keywords.items():
        if any(kw in combined for kw in kws):
            categories.append(cat)

    # æå–åŒºåŸŸ
    area = None
    area_keywords = ['æœé˜³åŒº', 'æµ·æ·€åŒº', 'ä¸œåŸåŒº', 'è¥¿åŸåŒº', 'ä¸°å°åŒº', 'çŸ³æ™¯å±±åŒº', 'ä¸‰é‡Œå±¯', 'å›½è´¸', 'ä¸­å…³æ‘']
    for area_kw in area_keywords:
        if area_kw in combined:
            area = area_kw
            break

    return {
        'name': shop_name or title[:30],
        'avg_price': avg_price,
        'score': score,
        'categories': categories,
        'area': area,
        'snippet': snippet[:200],
        'url': url,
        'source': 'dianping',
        'is_shop_page': '/shop/' in url,
    }


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å°é¥­å¡ - å¤§ä¼—ç‚¹è¯„æœç´¢')
    parser.add_argument('query', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--city', default='', help='åŸå¸‚')
    parser.add_argument('--max', type=int, default=50, help='æœ€å¤§ç»“æœæ•°')
    parser.add_argument('--json', action='store_true', help='JSON è¾“å‡º')
    args = parser.parse_args()

    results = search_dianping(args.query, args.city, max_results=args.max)

    if args.json:
        print(json.dumps(results, ensure_ascii=False, indent=2))
    else:
        print(f"ğŸ“Š æ‰¾åˆ° {len(results)} å®¶é¤å…\n")
        for i, r in enumerate(results[:10], 1):
            price = f"Â¥{r['avg_price']}" if r.get('avg_price') else ''
            score = f"â­{r['score']}" if r.get('score') else ''
            cats = '/'.join(r.get('categories', []))
            print(f"{i}. {r['name']}")
            if price or score or cats:
                print(f"   {' | '.join(p for p in [price, score, cats] if p)}")
            print()
