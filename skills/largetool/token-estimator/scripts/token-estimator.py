#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Token Estimator - Token æ¶ˆè€—é¢„ä¼°æ ¸å¿ƒä»£ç 
ç‰ˆæœ¬ï¼š1.0.0
åˆ›å»ºæ—¶é—´ï¼š2026-02-24
ä½œè€…ï¼šNeoï¼ˆå®‡å®™ç¥ç»ç³»ç»Ÿï¼‰
"""

import sys
import json
import argparse
from typing import Tuple, Dict, Optional

# å°è¯•å¯¼å…¥ tokenizer åº“
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False

try:
    from transformers import AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import dashscope
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False


class TokenEstimator:
    """Token æ¶ˆè€—é¢„ä¼°å™¨"""
    
    def __init__(self, model_name: str = "dashscope/qwen3.5-plus"):
        self.model_name = model_name
        self.tokenizer = self._get_tokenizer()
    
    def _get_tokenizer(self):
        """è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„ Tokenizer"""
        model_lower = self.model_name.lower()
        
        # Qwen/dashscope ç³»åˆ—
        if "qwen" in model_lower or "dashscope" in model_lower:
            if TRANSFORMERS_AVAILABLE:
                try:
                    print(f"ğŸ”§ ä½¿ç”¨ Qwen Tokenizer (transformers)", file=sys.stderr)
                    return AutoTokenizer.from_pretrained("Qwen/Qwen-7B")
                except Exception as e:
                    print(f"âš ï¸ Qwen Tokenizer åŠ è½½å¤±è´¥ï¼š{e}", file=sys.stderr)
            return self._fallback_estimator
        
        # OpenAI ç³»åˆ—
        elif "gpt" in model_lower or "openai" in model_lower:
            if TIKTOKEN_AVAILABLE:
                try:
                    encoding = tiktoken.encoding_for_model(model_lower.split('/')[-1])
                    print(f"ğŸ”§ ä½¿ç”¨ OpenAI Tokenizer (tiktoken)", file=sys.stderr)
                    return encoding.encode
                except Exception as e:
                    print(f"âš ï¸ OpenAI Tokenizer åŠ è½½å¤±è´¥ï¼š{e}", file=sys.stderr)
            if TIKTOKEN_AVAILABLE:
                return tiktoken.get_encoding("cl100k_base").encode
        
        # Gemini ç³»åˆ—
        elif "gemini" in model_lower:
            if TIKTOKEN_AVAILABLE:
                print(f"ğŸ”§ ä½¿ç”¨ Gemini Tokenizer (tiktoken/cl100k_base)", file=sys.stderr)
                return tiktoken.get_encoding("cl100k_base").encode
        
        # æœªçŸ¥æ¨¡å‹ï¼šé™çº§åˆ°å­—ç¬¦ä¼°ç®—
        print(f"âš ï¸ æœªçŸ¥æ¨¡å‹ '{self.model_name}'ï¼Œä½¿ç”¨å­—ç¬¦ä¼°ç®—", file=sys.stderr)
        return self._fallback_estimator
    
    def _fallback_estimator(self, text: str) -> list:
        """å­—ç¬¦ä¼°ç®—ï¼šä¸­æ–‡ 4 å­—â‰ˆ1 tokenï¼Œè‹±æ–‡ 4 å­—ç¬¦â‰ˆ1 token"""
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_chars = len([c for c in text if c.isascii() and c.isalnum()])
        # è¿”å›è™šæ‹Ÿçš„ token IDsï¼ˆä»…ç”¨äºè®¡æ•°ï¼‰
        return list(range((chinese_chars // 4) + (english_chars // 4)))
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
        if callable(self.tokenizer):
            tokens = self.tokenizer(text)
            if isinstance(tokens, list):
                return len(tokens)
            else:
                # transformers tokenizer è¿”å› dict
                return len(tokens.get('input_ids', []))
        else:
            return 0
    
    def estimate_output_tokens(self, input_length: int, text_type: str = "auto") -> Tuple[int, int]:
        """
        é¢„ä¼°è¾“å‡º token æ•°é‡
        
        è¿”å›ï¼š(min_output, max_output)
        """
        # æ ¹æ®è¾“å…¥é•¿åº¦å’Œæ–‡æœ¬ç±»å‹ä¼°ç®—
        if text_type == "auto":
            if input_length < 1000:
                text_type = "short"
            elif input_length < 5000:
                text_type = "medium"
            else:
                text_type = "long"
        
        # è¾“å‡º token ä¼°ç®—è§„åˆ™
        output_ranges = {
            "short": (200, 500),      # çŸ­æ–‡æœ¬
            "medium": (500, 1500),    # ä¸­æ–‡æœ¬
            "long": (1500, 3000),     # é•¿æ–‡æœ¬
            "code": (300, 1000),      # ä»£ç 
            "dialogue": (100, 300),   # å¯¹è¯
        }
        
        min_out, max_out = output_ranges.get(text_type, (500, 1500))
        return (min_out, max_out)
    
    def estimate_compression_savings(self, input_tokens: int) -> Dict:
        """é¢„ä¼° 4D å‹ç¼©åçš„èŠ‚çœæ•ˆæœ"""
        # åŸºäº T1-T7 å®éªŒæ•°æ®
        compression_rate = 0.70  # å¹³å‡ 70% èŠ‚çœ
        
        compressed_tokens = int(input_tokens * (1 - compression_rate))
        saved_tokens = input_tokens - compressed_tokens
        
        # æˆæœ¬ä¼°ç®—ï¼ˆæŒ‰ dashscope ä»·æ ¼ï¼‰
        cost_per_1k = 0.002  # $0.002 per 1k tokensï¼ˆç¤ºä¾‹ä»·æ ¼ï¼‰
        saved_cost = (saved_tokens / 1000) * cost_per_1k
        
        return {
            "original": input_tokens,
            "compressed": compressed_tokens,
            "saved": saved_tokens,
            "rate": compression_rate * 100,
            "cost_saved_usd": saved_cost,
            "cost_saved_cny": saved_cost * 7.2
        }
    
    def format_usage_meter(self, usage_data: Dict) -> str:
        """æ ¼å¼åŒ– Token æ°´è¡¨æ˜¾ç¤º"""
        used = usage_data.get("used", 0)
        quota = usage_data.get("quota", 18000)
        percentage = (used / quota * 100) if quota > 0 else 0
        
        # è¿›åº¦æ¡
        bar_length = 20
        filled = int(bar_length * percentage / 100)
        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
        
        return f"""
ğŸ’§ Token æ°´è¡¨ï¼ˆ{usage_data.get("period", "æœˆåº¦")}ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å·²ç”¨ï¼š{bar} {percentage:.1f}%
é…é¢ï¼š{used:,} / {quota:,}
å‰©ä½™ï¼š{quota - used:,} tokens"""


def detect_text_type(text: str) -> str:
    """è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬ç±»å‹"""
    # æ£€æµ‹ä»£ç 
    if any(kw in text for kw in ["def ", "function", "import ", "class ", "if __name__"]):
        return "code"
    
    # æ£€æµ‹å¯¹è¯
    if text.count("\n") < 3 and len(text) < 500:
        return "dialogue"
    
    # æ£€æµ‹ä¸­è‹±æ··åˆ
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    english_chars = len([c for c in text if c.isascii() and c.isalpha()])
    
    if chinese_chars > 0 and english_chars > 0:
        if english_chars / (chinese_chars + english_chars) > 0.3:
            return "mixed"
    
    # é»˜è®¤æŒ‰é•¿åº¦åˆ†ç±»
    if len(text) < 1000:
        return "short"
    elif len(text) < 5000:
        return "medium"
    else:
        return "long"


def print_estimation(estimator: TokenEstimator, text: str, with_compress: bool = False):
    """æ‰“å° Token é¢„ä¼°ç»“æœ"""
    
    # è®¡ç®—è¾“å…¥ tokens
    input_tokens = estimator.count_tokens(text)
    
    # æ£€æµ‹æ–‡æœ¬ç±»å‹
    text_type = detect_text_type(text)
    
    # é¢„ä¼°è¾“å‡º tokens
    min_output, max_output = estimator.estimate_output_tokens(input_tokens, text_type)
    
    # æ€»æ¶ˆè€—
    total_min = input_tokens + min_output
    total_max = input_tokens + max_output
    
    # æ‰“å°ç»“æœ
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚  ğŸ“Š Token æ¶ˆè€—é¢„ä¼°                      â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚  æ¨¡å‹ï¼š{estimator.model_name:<35}â”‚")
    print("â”‚                                         â”‚")
    print(f"â”‚  åŸæ–‡é•¿åº¦ï¼š{len(text):,} å­—".ljust(42) + "â”‚")
    print(f"â”‚  æ–‡æœ¬ç±»å‹ï¼š{text_type}".ljust(42) + "â”‚")
    print(f"â”‚  é¢„è®¡è¾“å…¥ï¼šçº¦ {input_tokens:,} tokens".ljust(42) + "â”‚")
    print(f"â”‚  é¢„è®¡è¾“å‡ºï¼šçº¦ {min_output:,}â€“{max_output:,} tokens".ljust(42) + "â”‚")
    print("â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚")
    print(f"â”‚  æ€»è®¡æ¶ˆè€—ï¼šçº¦ {total_min:,}â€“{total_max:,} tokens".ljust(42) + "â”‚")
    
    # 4D å‹ç¼©å»ºè®®
    if with_compress and input_tokens > 500:
        savings = estimator.estimate_compression_savings(input_tokens)
        compressed_total_min = savings["compressed"] + min_output
        compressed_total_max = savings["compressed"] + max_output
        
        print("â”‚                                         â”‚")
        print("â”‚  ğŸ’¡ å¯ç”¨ 4D å‹ç¼©åï¼š                     â”‚")
        print(f"â”‚     èŠ‚çœï¼šçº¦ {savings['saved']:,} tokens ({savings['rate']:.0f}%)".ljust(42) + "â”‚")
        print(f"â”‚     å®é™…ï¼šçº¦ {compressed_total_min:,}â€“{compressed_total_max:,} tokens".ljust(42) + "â”‚")
        print(f"â”‚     æˆæœ¬èŠ‚çœï¼šÂ¥{savings['cost_saved_cny']:.3f} CNY".ljust(42) + "â”‚")
    
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Token æ¶ˆè€—é¢„ä¼°å·¥å…·")
    parser.add_argument("text", nargs="?", help="è¦é¢„ä¼°çš„æ–‡æœ¬")
    parser.add_argument("--model", "-m", default="dashscope/qwen3.5-plus", 
                       help="æ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šdashscope/qwen3.5-plusï¼‰")
    parser.add_argument("--with-compress", "-c", action="store_true",
                       help="æ˜¾ç¤º 4D å‹ç¼©å»ºè®®")
    parser.add_argument("--usage", "-u", action="store_true",
                       help="æ˜¾ç¤º Token æ°´è¡¨ç”¨é‡")
    parser.add_argument("--json", "-j", action="store_true",
                       help="è¾“å‡º JSON æ ¼å¼")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æä¾›æ–‡æœ¬ï¼Œä» stdin è¯»å–
    if args.text:
        text = args.text
    elif not sys.stdin.isatty():
        text = sys.stdin.read()
    else:
        parser.print_help()
        sys.exit(1)
    
    # åˆ›å»ºé¢„ä¼°å™¨
    estimator = TokenEstimator(args.model)
    
    # è¾“å‡ºç»“æœ
    if args.json:
        # JSON è¾“å‡ºï¼ˆç”¨äºç¨‹åºè°ƒç”¨ï¼‰
        input_tokens = estimator.count_tokens(text)
        text_type = detect_text_type(text)
        min_out, max_out = estimator.estimate_output_tokens(input_tokens, text_type)
        
        result = {
            "model": args.model,
            "input_length": len(text),
            "input_tokens": input_tokens,
            "output_tokens_min": min_out,
            "output_tokens_max": max_out,
            "total_min": input_tokens + min_out,
            "total_max": input_tokens + max_out,
            "text_type": text_type
        }
        
        if args.with_compress and input_tokens > 500:
            result["compression"] = estimator.estimate_compression_savings(input_tokens)
        
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        # äººç±»å¯è¯»è¾“å‡º
        print_estimation(estimator, text, args.with_compress)


if __name__ == "__main__":
    main()
