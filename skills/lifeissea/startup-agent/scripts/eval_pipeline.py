#!/usr/bin/env python3
from __future__ import annotations  # Python 3.9 compatibility
"""
Eval Pipeline â€” LLM í‰ê°€ vs ì‹¤ì œ ì‹¬ì‚¬ê²°ê³¼ ë¹„êµ

Usage:
  # ì‹¤ì œ ê²°ê³¼ ë“±ë¡
  python3 eval_pipeline.py add --file plan.pdf --program TIPS --result pass --score 82

  # LLM í‰ê°€ ì‹¤í–‰ + ê²°ê³¼ ì €ì¥
  python3 eval_pipeline.py run --file plan.pdf [--model qwen3:8b]

  # ì •í™•ë„ ë¦¬í¬íŠ¸
  python3 eval_pipeline.py report

  # íŠ¹ì • íŒŒì¼ ë¹„êµ
  python3 eval_pipeline.py compare --file plan.pdf
"""

import argparse
import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent
GROUND_TRUTH = BASE_DIR / "ground_truth.jsonl"
EVAL_RESULTS = BASE_DIR / "eval_results.jsonl"
HISTORY = BASE_DIR / "history.jsonl"
EVALUATE_PY = BASE_DIR / "scripts" / "evaluate.py"

# --- helpers ---

def load_jsonl(path: Path) -> list[dict]:
    if not path.exists():
        return []
    entries = []
    for line in path.read_text().strip().split("\n"):
        line = line.strip()
        if line:
            entries.append(json.loads(line))
    return entries

def append_jsonl(path: Path, entry: dict):
    with open(path, "a") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

def find_ground_truth(filename: str) -> dict | None:
    for gt in load_jsonl(GROUND_TRUTH):
        if gt.get("file") == filename:
            return gt
    return None

def find_eval_scores(filename: str) -> list[dict]:
    """Find all LLM eval scores for a file from history.jsonl and eval_results.jsonl."""
    results = []
    for entry in load_jsonl(HISTORY):
        files = entry.get("files", [])
        if filename in files and entry.get("score") is not None:
            results.append(entry)
    for entry in load_jsonl(EVAL_RESULTS):
        if entry.get("file") == filename:
            results.append(entry)
    return results

# --- commands ---

def cmd_add(args):
    """Register actual review result."""
    entry = {
        "file": args.file,
        "program": args.program or "TIPS",
        "actual_result": args.result,  # pass/fail
        "actual_score": args.score,     # optional numeric
        "notes": args.notes or "",
        "added_at": datetime.now().isoformat(),
    }
    append_jsonl(GROUND_TRUTH, entry)
    print(f"âœ… Ground truth ë“±ë¡: {args.file} â†’ {args.result}" +
          (f" ({args.score}ì )" if args.score else ""))

def cmd_run(args):
    """Run LLM evaluation and store result."""
    if not os.path.exists(args.file):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {args.file}")
        sys.exit(1)

    model = args.model or "qwen3:8b"
    cmd = [sys.executable, str(EVALUATE_PY), "--mode", "evaluate",
           "--model", model, "--json", args.file]

    print(f"ğŸ” í‰ê°€ ì¤‘: {args.file} (model={model})")
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

    if result.returncode != 0:
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {result.stderr[:200]}")
        sys.exit(1)

    try:
        output = json.loads(result.stdout)
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {result.stdout[:200]}")
        sys.exit(1)

    score = output.get("score")
    entry = {
        "file": os.path.basename(args.file),
        "model": model,
        "llm_score": score,
        "timestamp": datetime.now().isoformat(),
        "mode": "evaluate",
    }
    append_jsonl(EVAL_RESULTS, entry)
    print(f"ğŸ“Š LLM ì ìˆ˜: {score}/100 â†’ eval_results.jsonl ì €ì¥")

    # Auto-compare if ground truth exists
    gt = find_ground_truth(os.path.basename(args.file))
    if gt:
        _print_comparison(os.path.basename(args.file), [entry], gt)

def cmd_compare(args):
    """Compare LLM scores vs ground truth for a file."""
    filename = os.path.basename(args.file)
    gt = find_ground_truth(filename)
    if not gt:
        print(f"âš ï¸ Ground truth ì—†ìŒ: {filename}")
        print(f"   â†’ eval_pipeline.py add --file {filename} --result pass/fail --score XX")
        return

    evals = find_eval_scores(filename)
    if not evals:
        print(f"âš ï¸ í‰ê°€ ê¸°ë¡ ì—†ìŒ: {filename}")
        return

    _print_comparison(filename, evals, gt)

def _print_comparison(filename: str, evals: list[dict], gt: dict):
    """Print comparison between LLM evals and ground truth."""
    actual = gt.get("actual_score")
    actual_result = gt.get("actual_result", "?")
    program = gt.get("program", "?")

    print(f"\n{'='*50}")
    print(f"ğŸ“‹ {filename} ({program})")
    print(f"   ì‹¤ì œ: {actual_result.upper()}" +
          (f" ({actual}ì )" if actual else ""))
    print(f"{'â”€'*50}")

    for ev in evals:
        llm_score = ev.get("llm_score") or ev.get("score")
        model = ev.get("model", "?")
        if llm_score is not None and actual is not None:
            diff = llm_score - actual
            sign = "+" if diff > 0 else ""
            accuracy_icon = "âœ…" if abs(diff) <= 10 else "âš ï¸" if abs(diff) <= 20 else "âŒ"
            print(f"   {accuracy_icon} LLM({model}): {llm_score}ì  (ì°¨ì´: {sign}{diff})")
        elif llm_score is not None:
            # No actual score, check pass/fail prediction
            predicted = "pass" if llm_score >= 70 else "fail"
            match = "âœ…" if predicted == actual_result else "âŒ"
            print(f"   {match} LLM({model}): {llm_score}ì  â†’ ì˜ˆì¸¡={predicted}, ì‹¤ì œ={actual_result}")

    print(f"{'='*50}\n")

def _compute_confusion(gt_entries: list[dict], eval_getter) -> dict:
    """Compute confusion matrix stats. Returns dict keyed by program (and '__all__')."""
    from collections import defaultdict
    programs = defaultdict(lambda: {"tp": 0, "fp": 0, "tn": 0, "fn": 0,
                                     "score_diffs": [], "total": 0})

    for gt in gt_entries:
        filename = gt["file"]
        evals = eval_getter(filename)
        actual_result = gt.get("actual_result")
        actual_score = gt.get("actual_score")
        program = gt.get("program", "ê¸°íƒ€")

        for ev in evals:
            llm_score = ev.get("llm_score") or ev.get("score")
            if llm_score is None:
                continue
            predicted = "pass" if llm_score >= 70 else "fail"

            for key in [program, "__all__"]:
                bucket = programs[key]
                bucket["total"] += 1
                if actual_score is not None:
                    bucket["score_diffs"].append(abs(llm_score - actual_score))
                if actual_result:
                    if predicted == "pass" and actual_result == "pass":
                        bucket["tp"] += 1
                    elif predicted == "pass" and actual_result == "fail":
                        bucket["fp"] += 1
                    elif predicted == "fail" and actual_result == "fail":
                        bucket["tn"] += 1
                    elif predicted == "fail" and actual_result == "pass":
                        bucket["fn"] += 1

    return dict(programs)


def _print_confusion_block(label: str, s: dict):
    """Print confusion matrix + metrics for one bucket."""
    tp, fp, tn, fn = s["tp"], s["fp"], s["tn"], s["fn"]
    total_pf = tp + fp + tn + fn
    accuracy = (tp + tn) / total_pf * 100 if total_pf else 0
    precision = tp / (tp + fp) if (tp + fp) else 0
    recall = tp / (tp + fn) if (tp + fn) else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0

    print(f"\nâ”Œâ”€ {label} (n={total_pf})")
    print(f"â”‚  Confusion Matrix:")
    print(f"â”‚              ì˜ˆì¸¡Pass  ì˜ˆì¸¡Fail")
    print(f"â”‚  ì‹¤ì œPass      {tp:>4}     {fn:>4}")
    print(f"â”‚  ì‹¤ì œFail      {fp:>4}     {tn:>4}")
    print(f"â”‚")
    print(f"â”‚  Accuracy:  {accuracy:5.1f}%")
    print(f"â”‚  Precision: {precision:5.3f}")
    print(f"â”‚  Recall:    {recall:5.3f}")
    print(f"â”‚  F1 Score:  {f1:5.3f}")

    diffs = s["score_diffs"]
    if diffs:
        avg_d = sum(diffs) / len(diffs)
        within_10 = sum(1 for d in diffs if d <= 10)
        print(f"â”‚  í‰ê·  ì ìˆ˜ ì°¨ì´: {avg_d:.1f}ì   (Â±10ì  ì´ë‚´: {within_10}/{len(diffs)})")

    print(f"â””{'â”€'*50}")


def cmd_report(args):
    """Generate accuracy report across all files with ground truth."""
    gt_entries = load_jsonl(GROUND_TRUTH)
    if not gt_entries:
        print("âš ï¸ Ground truth ë°ì´í„° ì—†ìŒ. ë¨¼ì € ì‹¤ì œ ì‹¬ì‚¬ ê²°ê³¼ë¥¼ ë“±ë¡í•˜ì„¸ìš”:")
        print("   python3 eval_pipeline.py add --file plan.pdf --result pass --score 82")
        return

    # Count by program
    from collections import Counter
    program_counts = Counter(gt.get("program", "ê¸°íƒ€") for gt in gt_entries)

    print(f"\n{'='*60}")
    print(f"ğŸ“Š Eval Pipeline ì •í™•ë„ ë¦¬í¬íŠ¸")
    print(f"   Ground Truth: {len(gt_entries)}ê±´")
    for prog, cnt in sorted(program_counts.items()):
        pass_cnt = sum(1 for g in gt_entries if g.get("program") == prog and g.get("actual_result") == "pass")
        fail_cnt = cnt - pass_cnt
        print(f"     {prog}: {cnt}ê±´ (í•©ê²© {pass_cnt} / ë¶ˆí•©ê²© {fail_cnt})")
    print(f"{'='*60}")

    # Check if any evals exist
    stats = _compute_confusion(gt_entries, find_eval_scores)
    has_evals = stats.get("__all__", {}).get("total", 0) > 0

    if not has_evals:
        # No eval results yet â€” show ground truth summary only
        print("\nâ³ LLM í‰ê°€ ê²°ê³¼ ì—†ìŒ â€” ground truthë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")
        print("   í‰ê°€ ì‹¤í–‰: python3 eval_pipeline.py run --file <plan.pdf>")

        # Show score distribution per program
        for prog in sorted(program_counts.keys()):
            entries = [g for g in gt_entries if g.get("program") == prog]
            scores = [g["actual_score"] for g in entries if g.get("actual_score") is not None]
            if scores:
                avg_s = sum(scores) / len(scores)
                print(f"\n   {prog}: í‰ê·  {avg_s:.1f}ì  (ë²”ìœ„ {min(scores)}â€“{max(scores)})")
                pass_scores = [g["actual_score"] for g in entries if g.get("actual_result") == "pass" and g.get("actual_score")]
                fail_scores = [g["actual_score"] for g in entries if g.get("actual_result") == "fail" and g.get("actual_score")]
                if pass_scores:
                    print(f"     í•©ê²© í‰ê· : {sum(pass_scores)/len(pass_scores):.1f}ì ")
                if fail_scores:
                    print(f"     ë¶ˆí•©ê²© í‰ê· : {sum(fail_scores)/len(fail_scores):.1f}ì ")

        print(f"\n{'='*60}\n")
        return

    # Per-file comparison (verbose mode)
    if getattr(args, 'verbose', False):
        for gt in gt_entries:
            evals = find_eval_scores(gt["file"])
            if evals:
                _print_comparison(gt["file"], evals, gt)

    # Per-program breakdown
    for prog in sorted(program_counts.keys()):
        if prog in stats:
            _print_confusion_block(prog, stats[prog])

    # Overall
    if "__all__" in stats:
        _print_confusion_block("ì „ì²´ (Overall)", stats["__all__"])

    # Recommendations
    all_s = stats.get("__all__", {})
    diffs = all_s.get("score_diffs", [])
    if diffs and sum(diffs) / len(diffs) > 15:
        print("\nğŸ’¡ í‰ê·  ì°¨ì´ 15ì  ì´ˆê³¼ â†’ í”„ë¡¬í”„íŠ¸ íŠœë‹ ê¶Œì¥")
        print("   ì°¸ê³ : references/ ë””ë ‰í† ë¦¬ì— ì‹¬ì‚¬ê¸°ì¤€ì„œ ì¶”ê°€ ì‹œ ì •í™•ë„ í–¥ìƒ ê¸°ëŒ€")

    f1 = 0
    tp, fp, fn = all_s.get("tp", 0), all_s.get("fp", 0), all_s.get("fn", 0)
    if tp + fp and tp + fn:
        prec = tp / (tp + fp)
        rec = tp / (tp + fn)
        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) else 0
    if f1 and f1 < 0.7:
        print(f"\nâš ï¸ F1 = {f1:.3f} < 0.7 â€” ëª¨ë¸/í”„ë¡¬í”„íŠ¸ ê°œì„  í•„ìš”")

    print(f"\n{'='*60}\n")

# --- main ---

def main():
    parser = argparse.ArgumentParser(description="Eval Pipeline: LLM í‰ê°€ vs ì‹¤ì œ ì‹¬ì‚¬ ë¹„êµ")
    sub = parser.add_subparsers(dest="command")

    # add
    p_add = sub.add_parser("add", help="ì‹¤ì œ ì‹¬ì‚¬ ê²°ê³¼ ë“±ë¡")
    p_add.add_argument("--file", required=True, help="íŒŒì¼ëª…")
    p_add.add_argument("--program", default="TIPS", help="ì§€ì›ì‚¬ì—…ëª…")
    p_add.add_argument("--result", required=True, choices=["pass", "fail"])
    p_add.add_argument("--score", type=int, help="ì‹¤ì œ ì ìˆ˜ (ì„ íƒ)")
    p_add.add_argument("--notes", help="ë¹„ê³ ")

    # run
    p_run = sub.add_parser("run", help="LLM í‰ê°€ ì‹¤í–‰ + ì €ì¥")
    p_run.add_argument("--file", required=True, help="ì‚¬ì—…ê³„íšì„œ íŒŒì¼")
    p_run.add_argument("--model", default="qwen3:8b")

    # compare
    p_cmp = sub.add_parser("compare", help="íŠ¹ì • íŒŒì¼ ë¹„êµ")
    p_cmp.add_argument("--file", required=True)

    # report
    p_report = sub.add_parser("report", help="ì „ì²´ ì •í™•ë„ ë¦¬í¬íŠ¸")
    p_report.add_argument("--verbose", "-v", action="store_true", help="íŒŒì¼ë³„ ìƒì„¸ ë¹„êµ í¬í•¨")

    args = parser.parse_args()
    if not args.command:
        parser.print_help()
        sys.exit(1)

    {"add": cmd_add, "run": cmd_run, "compare": cmd_compare, "report": cmd_report}[args.command](args)

if __name__ == "__main__":
    main()
